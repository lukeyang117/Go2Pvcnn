"""Observation functions for Go2 PVCNN locomotion."""

from __future__ import annotations

import torch
from typing import TYPE_CHECKING, Dict

from isaaclab.assets import Articulation
from isaaclab.managers import SceneEntityCfg
from isaaclab.sensors import RayCaster, ContactSensor
from isaaclab.envs import mdp as isaac_mdp

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv

# Import cost map generators
from go2_pvcnn.mdp.cost_map import CostMapGenerator
from go2_pvcnn.mdp.cost_map_new import TeacherCostMapGenerator


def binary_contact_state(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg,
    threshold: float = 1.0,
) -> torch.Tensor:
    """Binary contact state for each foot (1 if in contact, -1 if not).
    
    This matches the ABS training observation format: contact_filt.float() * 2 - 1.0
    
    Args:
        env: The environment.
        sensor_cfg: The SceneEntity associated with a ContactSensor (with body_names filtering).
        threshold: Contact force threshold in Newtons. Defaults to 1.0.
    
    Returns:
        Binary contact state: (batch, num_bodies). Values are +1 (contact) or -1 (no contact).
    """
    # Extract the ContactSensor
    sensor: ContactSensor = env.scene.sensors[sensor_cfg.name]
    
    # Get contact forces (net forces in world frame)
    # Shape: (num_envs, num_bodies, 3)
    contact_forces = sensor.data.net_forces_w
    
    # Filter to specific bodies if body_ids is specified
    if sensor_cfg.body_ids is not None and len(sensor_cfg.body_ids) > 0:
        contact_forces = contact_forces[:, sensor_cfg.body_ids, :]
    
    # Compute force magnitude for each body
    # Shape: (num_envs, num_filtered_bodies)
    force_magnitude = torch.norm(contact_forces, dim=-1)
    
    # Binary contact: True if force > threshold
    in_contact = force_magnitude > threshold
    
    # Convert to ABS format: +1 for contact, -1 for no contact
    # (contact_filt.float() * 2 - 1.0)
    binary_state = in_contact.float() * 2.0 - 1.0
    
    return binary_state


def ray2d_obstacle_distance(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg,
    log2_scale: bool = True,
    max_distance: float = 6.0,
) -> torch.Tensor:
    """Calculate 2D ray distances from RayCaster sensor.
    
    This function computes the Euclidean distance from the sensor to the hit points
    in the XY plane (ignoring Z), similar to the ray2d sensor in Isaac Gym.
    
    Args:
        env: The environment.
        sensor_cfg: The SceneEntity associated with a RayCaster sensor.
        log2_scale: Whether to apply log2 scaling to distances. Defaults to True.
        max_distance: Maximum distance for rays that don't hit anything. Defaults to 6.0m.
    
    Returns:
        Ray distances in XY plane. Shape is (num_envs, num_rays).
        If log2_scale=True, applies torch.log2() to the distances.
    """
    # Extract the RayCaster sensor
    sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    
    # Get sensor position and ray hit points (both in world frame)
    sensor_pos = sensor.data.pos_w  # Shape: (num_envs, 3)
    ray_hits = sensor.data.ray_hits_w  # Shape: (num_envs, num_rays, 3)
    
    # Compute 2D distance (XY plane only)
    # Expand sensor_pos to match ray_hits shape: (num_envs, 1, 3) -> (num_envs, num_rays, 3)
    sensor_pos_expanded = sensor_pos.unsqueeze(1)
    
    # Calculate distance in XY plane
    xy_diff = ray_hits[..., :2] - sensor_pos_expanded[..., :2]  # Shape: (num_envs, num_rays, 2)
    distances = torch.norm(xy_diff, dim=-1)  # Shape: (num_envs, num_rays)
    
    # Handle rays that didn't hit anything or have invalid data
    # Clamp to avoid log2(0) and handle initialization issues
    distances = torch.clamp(distances, min=0.1, max=max_distance)
    
    # Replace any NaN/inf values with max_distance
    distances = torch.nan_to_num(distances, nan=max_distance, posinf=max_distance, neginf=0.1)
    
    # Apply log2 scaling if requested (as in ABS training)
    if log2_scale:
        distances = torch.log2(distances)
    
    return distances


def pvcnn_features_with_cost_map(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg = SceneEntityCfg("lidar_sensor"),
    height_scanner_cfg: SceneEntityCfg = SceneEntityCfg("height_scanner"),
) -> torch.Tensor:
    """Generate cost map from LiDAR semantic segmentation + height scanner.
    
    Data flow:
        LiDARç‚¹äº‘ â†’ PVCNNæ¨ç† â†’ è¯­ä¹‰åˆ†å‰²ç»“æœ + é«˜ç¨‹å›¾ â†’ ç”Ÿæˆä»£ä»·åœ°å›¾
    
    The cost map is generated by:
        1. PVCNN performs semantic segmentation on LiDAR point cloud
        2. Height scanner provides ground truth height map (16Ã—16 grid)
        3. CostMapGenerator combines semantic + height data:
            - Distance cost: proximity to obstacles (from semantic labels)
            - Gradient cost: terrain steepness (from height scanner)
            - Confidence cost: semantic uncertainty (from PVCNN confidence)
    
    Returns:
        dual_channel_map: (batch, 512) - Flattened dual-channel map (2 channels Ã— 16Ã—16 = 512)
            - Channel 0: cost_map (ä»£ä»·åœ°å›¾ï¼Œç»¼åˆéšœç¢ç‰©ã€è·ç¦»ã€æ¢¯åº¦ã€é«˜åº¦çš„åŠ æƒä»£ä»·)
            - Channel 1: height_map (é«˜ç¨‹å›¾ï¼ŒåŸå§‹é«˜åº¦ä¿¡æ¯)
    
    Note: This function stores point_cloud, cost_map, and height_map in env for reward calculation.
    """
    # Initialize debug counters and cost map generator
    if not hasattr(pvcnn_features_with_cost_map, '_debug_printed'):
        pvcnn_features_with_cost_map._debug_printed = False
        pvcnn_features_with_cost_map._call_count = 0
        
        # Initialize cost map generator (singleton)
        # Match height_scanner size: resolution=0.1, size=[1.5, 1.5] = 16Ã—16 grid (including boundaries)
        pvcnn_features_with_cost_map.cost_map_generator = CostMapGenerator(
            grid_size=(16, 16),  # åŒ¹é…height_scannerçš„ç½‘æ ¼å°ºå¯¸ (GridPatternCfgåŒ…å«è¾¹ç•Œç‚¹)
            grid_resolution=0.1,  # åŒ¹é…height_scannerçš„åˆ†è¾¨ç‡
            x_range=(-0.75, 0.75),  # size[0]/2 = 1.5/2 = 0.75
            y_range=(-0.75, 0.75),  # size[1]/2 = 1.5/2 = 0.75
            obstacle_class_ids=[1, 2, 3],  # Class 0=Terrain, 1=CrackerBox, 2=SugarBox, 3=TomatoSoupCan
            device=env.device,
            # ä»£ä»·æƒé‡å‚æ•°
            weight_obstacle=1.0,
            weight_distance=0.5,
            weight_gradient=1.0,
            weight_abs_height=0.3
        )
    
    # Increment call counter
    pvcnn_features_with_cost_map._call_count += 1
    
    # Get LiDAR sensor
    lidar_sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    
    # Debug: Print sensor type and configuration (only once)
    if not pvcnn_features_with_cost_map._debug_printed:
        print(f"\n{'='*80}")
        print(f"[PVCNN+CostMap DEBUG] LiDAR Sensor Configuration:")
        print(f"  - Sensor type: {type(lidar_sensor).__name__}")
        print(f"  - Number of rays: {lidar_sensor.num_rays}")
        print(f"  - Cost map grid: 16Ã—16 (matching height_scanner)")
        print(f"  - Cost map range: X=[-0.75, 0.75]m, Y=[-0.75, 0.75]m")
        print(f"  - Using Isaac Lab pointcloud API with yaw-alignment")
        
        # ========================================
        # ğŸ†• æ£€æŸ¥è¯­ä¹‰æ ‡ç­¾æ”¯æŒ
        # ========================================
        if hasattr(lidar_sensor.data, 'semantic_labels'):
            print(f"  - âœ… Semantic labels: SUPPORTED")
            if hasattr(lidar_sensor, 'semantic_class_names'):
                print(f"  - Semantic classes: {lidar_sensor.semantic_class_names}")
        
        print(f"{'='*80}\n")
        pvcnn_features_with_cost_map._debug_printed = True
    
    # Get point cloud data in yaw-aligned frame
    # Isaac Lab respects ray_alignment="yaw" and generates pointcloud accordingly
    point_cloud_raw = lidar_sensor.data.pointcloud  # (num_envs, num_rays, 3) - yaw-aligned point cloud
    
    # Debug: Check point cloud data and detect Inf values BEFORE any processing
    if pvcnn_features_with_cost_map._call_count % 100 == 1:
        print(f"\n[PVCNN DEBUG] Raw Point Cloud Data (BEFORE processing):")
        print(f"  - point_cloud_raw shape: {point_cloud_raw.shape}")
        print(f"  - Coordinate frame: yaw-aligned (from Isaac Lab pointcloud)")
        
        # Count Inf values in ORIGINAL data
        has_inf = torch.isinf(point_cloud_raw).any(dim=-1)  # (num_envs, num_rays)
        total_rays = has_inf.numel()
        inf_rays = has_inf.sum().item()
        print(f"  - Rays with Inf (ORIGINAL): {inf_rays}/{total_rays} ({100*inf_rays/total_rays:.2f}%)")
        
        # Check for valid hits
        valid_rays = total_rays - inf_rays
        if valid_rays > 0:
            # Get valid points for statistics
            valid_points = point_cloud_raw[~has_inf]
            print(f"  - Valid rays: {valid_rays} ({100*valid_rays/total_rays:.2f}%)")
            print(f"  - Valid points range: X[{valid_points[:,0].min():.2f}, {valid_points[:,0].max():.2f}], "
                  f"Y[{valid_points[:,1].min():.2f}, {valid_points[:,1].max():.2f}], "
                  f"Z[{valid_points[:,2].min():.2f}, {valid_points[:,2].max():.2f}]")
        
        # ========================================
        # ğŸ†• è¯­ä¹‰æ ‡ç­¾è°ƒè¯•ä¿¡æ¯
        # ========================================
        if hasattr(lidar_sensor.data, 'semantic_labels'):
            semantic_labels = lidar_sensor.data.semantic_labels #shape: (num_envs, num_rays)
            print(f"\n[SEMANTIC DEBUG] Semantic Labels Information:")
            print(f"  - semantic_labels shape: {semantic_labels.shape}")
            print(f"  - semantic_labels dtype: {semantic_labels.dtype}")
            print(f"  - Value range: [{semantic_labels.min().item()}, {semantic_labels.max().item()}]")
            
            # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ•°é‡
            unique_labels, counts = torch.unique(semantic_labels, return_counts=True)
            print(f"  - Unique labels found: {unique_labels.tolist()}")
            print(f"  - Label distribution:")
            for label, count in zip(unique_labels.tolist(), counts.tolist()):
                percentage = 100 * count / semantic_labels.numel()
                if label == -1:
                    label_name = "no_hit"
                elif label == 0:
                    label_name = "terrain"
                elif hasattr(lidar_sensor, 'semantic_class_names') and label < len(lidar_sensor.semantic_class_names):
                    label_name = lidar_sensor.semantic_class_names[label]
                else:
                    label_name = f"object_{label}"
                print(f"    - Label {label:2d} ({label_name:20s}): {count:6d} rays ({percentage:5.2f}%)")
            
            # æ£€æŸ¥ hit_mesh_source
            if hasattr(lidar_sensor.data, 'hit_mesh_source'):
                hit_mesh_source = lidar_sensor.data.hit_mesh_source
                print(f"\n  - hit_mesh_source shape: {hit_mesh_source.shape}")
                unique_sources, source_counts = torch.unique(hit_mesh_source, return_counts=True)
                print(f"  - Hit sources:")
                for source, count in zip(unique_sources.tolist(), source_counts.tolist()):
                    percentage = 100 * count / hit_mesh_source.numel()
                    source_name = {-1: "no_hit", 0: "static_mesh", 1: "dynamic_mesh"}.get(source, f"unknown_{source}")
                    print(f"    - Source {source:2d} ({source_name:15s}): {count:6d} rays ({percentage:5.2f}%)")
        else:
            print(f"\n[SEMANTIC DEBUG] âŒ semantic_labels not available in sensor data")
    
    # Isaac Lab's pointcloud respects ray_alignment="yaw" - already in yaw-aligned frame
    # Create comprehensive mask for valid points (not NaN, not Inf, not too close to origin)
    valid_mask = (~torch.isnan(point_cloud_raw).any(dim=-1) & 
                  ~torch.isinf(point_cloud_raw).any(dim=-1))  # (num_envs, num_rays)
    
    # Filter out points too close to origin (likely invalid, at least 10cm)
    distance_from_origin = torch.norm(point_cloud_raw, dim=2)
    valid_mask = valid_mask & (distance_from_origin > 0.1)
    
    method_used = "Isaac Lab pointcloud (yaw-aligned)"
    
    # Debug statistics (print every 100 calls)
    if pvcnn_features_with_cost_map._call_count % 100 == 1:
        valid_count = valid_mask.sum(dim=1)
        total_rays = point_cloud_raw.shape[1]
        
        print(f"\n[PVCNN DEBUG - Call {pvcnn_features_with_cost_map._call_count}] Point Cloud Filtering:")
        print(f"  - Method: {method_used}")
        print(f"  - Total rays: {total_rays}")
        print(f"  - Valid points per env: min={valid_count.min()}, max={valid_count.max()}, mean={valid_count.float().mean():.1f}")
        
        # Statistics on valid points only
        all_valid_points = point_cloud_raw[valid_mask]
        if all_valid_points.shape[0] > 0:
            print(f"  - Valid points range: X[{all_valid_points[:,0].min():.3f}, {all_valid_points[:,0].max():.3f}], "
                  f"Y[{all_valid_points[:,1].min():.3f}, {all_valid_points[:,1].max():.3f}], "
                  f"Z[{all_valid_points[:,2].min():.3f}, {all_valid_points[:,2].max():.3f}]")
    
    # ========================================
    # Sample valid points to target size (2046 points)
    # ========================================
    target_num_points = 2046
    batch_size = point_cloud_raw.shape[0]
    
    # åŒæ—¶å¤„ç†ç‚¹äº‘å’Œè¯­ä¹‰æ ‡ç­¾ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    raw_semantic_labels = None
    if hasattr(lidar_sensor.data, 'semantic_labels'):
        raw_semantic_labels = lidar_sensor.data.semantic_labels  # (num_envs, num_rays)
    
    # Process each environment separately to handle different valid point counts
    processed_point_clouds = []
    processed_labels = []
    
    for b in range(batch_size):
        valid_points = point_cloud_raw[b][valid_mask[b]]  # ç›´æ¥ä»rawæå–validç‚¹ (num_valid, 3)
        num_valid = valid_points.shape[0]
        
        # åŒæ—¶æå–valid semantic labels
        if raw_semantic_labels is not None:
            valid_labels = raw_semantic_labels[b][valid_mask[b]]  # (num_valid,)
        
        if num_valid == 0:
            # No valid points - create dummy points in a small sphere
            dummy_points = torch.randn(target_num_points, 3, device=point_cloud_raw.device) * 0.5
            processed_point_clouds.append(dummy_points)
            
            if raw_semantic_labels is not None:
                dummy_labels = torch.zeros(target_num_points, dtype=torch.long, device=point_cloud_raw.device)
                processed_labels.append(dummy_labels)
                
        elif num_valid < target_num_points:
            # Not enough points: duplicate existing points
            num_repeats = (target_num_points + num_valid - 1) // num_valid
            repeated_points = valid_points.repeat(num_repeats, 1)[:target_num_points]
            processed_point_clouds.append(repeated_points)
            
            if raw_semantic_labels is not None:
                repeated_labels = valid_labels.repeat(num_repeats)[:target_num_points]
                processed_labels.append(repeated_labels)
            
        
        elif num_valid > target_num_points:
            # Too many points: use farthest point sampling
            distances = torch.norm(valid_points, dim=1)
            _, furthest_indices = torch.topk(distances, k=target_num_points, largest=True)
            sampled_points = valid_points[furthest_indices]
            processed_point_clouds.append(sampled_points)
            
            if raw_semantic_labels is not None:
                sampled_labels = valid_labels[furthest_indices]
                processed_labels.append(sampled_labels)
            
        
        else:
            # Exactly right number
            processed_point_clouds.append(valid_points)
            if raw_semantic_labels is not None:
                processed_labels.append(valid_labels)
    
    # Stack all environments
    point_cloud = torch.stack(processed_point_clouds, dim=0)  # (batch, target_num_points, 3)
    
    # Stack semantic labels if available
    semantic_labels = None
    if raw_semantic_labels is not None:
        semantic_labels = torch.stack(processed_labels, dim=0)  # (batch, target_num_points)
    
    if pvcnn_features_with_cost_map._call_count % 100 == 1:
        print(f"  - Final point cloud shape: {point_cloud.shape}")
        print(f"  - Final stats: min={point_cloud.min():.3f}, max={point_cloud.max():.3f}, mean={point_cloud.mean():.3f}")
        # Check for remaining invalid values
        if torch.isnan(point_cloud).any() or torch.isinf(point_cloud).any():
            print(f"  [ERROR] Still have NaN/Inf after processing!")
        
        if semantic_labels is not None:
            print(f"\n[SEMANTIC DEBUG] Processed semantic labels:")
            print(f"  - Shape: {semantic_labels.shape}")
            print(f"  - Unique labels: {torch.unique(semantic_labels).tolist()}")
            for label_id in torch.unique(semantic_labels):
                count = (semantic_labels == label_id).sum().item()
                percentage = 100 * count / semantic_labels.numel()
                print(f"    - Label {label_id}: {count} points ({percentage:.2f}%)")
    
    # Point cloud ready for PVCNN (XYZ only, 3 channels)
    # Model adapted to accept 3-channel input
    
    # Get PVCNN wrapper from unwrapped environment (injected by RslRlPvcnnEnvWrapper)
    # NOTE: During gym.make(), the environment may call reset() before wrapper is injected
    # So we need to handle the case where wrapper doesn't exist yet
    pvcnn_wrapper = getattr(env.unwrapped, 'pvcnn_wrapper', None)
    
    if pvcnn_wrapper is not None:
        if pvcnn_features_with_cost_map._call_count % 100 == 1:
            print(f"[PVCNN DEBUG] Using PVCNN wrapper from env.unwrapped.pvcnn_wrapper")
            print(f"[PVCNN DEBUG] Wrapper device: {pvcnn_wrapper.device}")
            print(f"[PVCNN DEBUG] Point cloud device: {point_cloud.device}")
        
        # CRITICAL: Ensure point cloud is contiguous in memory to prevent CUDA errors
        if not point_cloud.is_contiguous():
            point_cloud = point_cloud.contiguous()
        
        # Extract features using PVCNN (this gives us per-point semantic segmentation)
        with torch.no_grad():
            semantic_features = pvcnn_wrapper.extract_features(point_cloud)  # (batch, num_points, num_classes)
        
        # Debug: Print PVCNN output statistics
        if pvcnn_features_with_cost_map._call_count % 100 == 1:
            print(f"\n[PVCNN DEBUG] PVCNN Semantic Segmentation:")
            print(f"  - Shape: {semantic_features.shape}")
            print(f"  - Range: [{semantic_features.min():.6f}, {semantic_features.max():.6f}]")
            print(f"  - Mean: {semantic_features.mean():.6f}, Std: {semantic_features.std():.6f}")
        
        # ========================================
        # Get Height Map from Height Scanner
        # ========================================
        
        # Get height scan from height scanner sensor (16Ã—16 grid)
        height_scan = isaac_mdp.height_scan(env, sensor_cfg=height_scanner_cfg)  # (batch, 256)
        height_map_2d = height_scan.reshape(env.num_envs, 16, 16)  # (batch, H, W)
        
        if pvcnn_features_with_cost_map._call_count % 100 == 1:
            print(f"\n[HEIGHT SCANNER DEBUG] Height Map:")
            print(f"  - Shape: {height_map_2d.shape}")
            print(f"  - Range: [{height_map_2d.min():.3f}, {height_map_2d.max():.3f}]")
            print(f"  - Mean: {height_map_2d.mean():.3f}, Std: {height_map_2d.std():.3f}")
        
        # ========================================
        # Generate Cost Map from PVCNN output + Height Map
        # ========================================
        
        # Transpose semantic features to (batch, num_classes, num_points)
        semantic_logits = semantic_features.transpose(1, 2)
        
        # Compute semantic confidence and predicted classes
        semantic_probs = torch.softmax(semantic_logits, dim=1)
        semantic_confidence, pred_classes = semantic_probs.max(dim=1)  # (batch, num_points), (batch, num_points)
        
        # Generate cost map using the CostMapGenerator
        # CostMapGenerator will:
        # 1. Project 3D points to 2D grid using point_xyz (x, y) matching height_scanner range
        # 2. Use height_map from height scanner to compute gradient cost
        # 3. Use pred_classes to identify obstacles and compute distance cost
        # 4. Use semantic_confidence to compute uncertainty cost
        cost_map_2d = pvcnn_features_with_cost_map.cost_map_generator.generate_cost_map(
            point_xyz=point_cloud,  # (batch, num_points, 3)
            pred_classes=pred_classes,  # (batch, num_points) - predicted class indices
            semantic_confidence=semantic_confidence,  # (batch, num_points)
            height_map=height_map_2d,  # (batch, H, W) - from height scanner
        )
        
        # Flatten cost_map and height_map
        cost_map_flat = cost_map_2d.reshape(cost_map_2d.shape[0], -1)  # (batch, 256)
        height_map_flat = height_map_2d.reshape(height_map_2d.shape[0], -1)  # (batch, 256)
        
        # Concatenate into dual-channel map
        dual_channel_flat = torch.cat([cost_map_flat, height_map_flat], dim=1)  # (batch, 512)
        
        if pvcnn_features_with_cost_map._call_count % 100 == 1:
            print(f"\n[DualChannel DEBUG] Generated Dual-Channel Map:")
            print(f"  - cost_map_flat shape: {cost_map_flat.shape}")
            print(f"  - height_map_flat shape: {height_map_flat.shape}")
            print(f"  - dual_channel_flat shape: {dual_channel_flat.shape}")
            print(f"  - Cost map range: [{cost_map_flat.min():.6f}, {cost_map_flat.max():.6f}]")
            print(f"  - Height map range: [{height_map_flat.min():.6f}, {height_map_flat.max():.6f}]")
        
        # Store cost_map and height_map in env for reward calculation
        env.unwrapped.last_cost_map_2d = cost_map_2d.detach()  # (batch, H, W)
        env.unwrapped.last_height_map_2d = height_map_2d.detach()  # (batch, H, W)
        env.unwrapped.last_point_cloud = point_cloud.detach()
        env.unwrapped.last_semantic_labels = semantic_labels.detach() if semantic_labels is not None else None
        
        # Return dual-channel map
        return dual_channel_flat
    else:
        # PVCNN wrapper not found - this should not happen if wrapper is properly injected
        print(f"[ERROR] PVCNN wrapper not found in env.unwrapped.pvcnn_wrapper!")
        
        # Fallback: return zeros for dual-channel map
        dual_channel_dim = 2 * 16 * 16  # 2 channels Ã— 16 Ã— 16 grid = 512
        
        env.unwrapped.last_cost_map_2d = None
        env.unwrapped.last_height_map_2d = None
        env.unwrapped.last_point_cloud = None
        env.unwrapped.last_semantic_labels = None
        
        return torch.zeros(env.num_envs, dual_channel_dim, device=env.device)


def goal_based_velocity_commands(
    env: ManagerBasedRLEnv, 
    goal_distance: float = 5.0,
    max_speed: float = 1.0,
    asset_cfg: SceneEntityCfg = SceneEntityCfg("robot")
) -> torch.Tensor:
    """Generate velocity commands based on goal position.
    
    Instead of random sampling, this function calculates velocity commands
    that guide the robot towards a goal position ahead.
    
    Args:
        env: The environment instance.
        goal_distance: Distance to the goal in X direction (meters). Default: 5.0m
        max_speed: Maximum linear velocity (m/s). Default: 1.0
        asset_cfg: The scene entity configuration for the robot.
    
    Returns:
        vel_commands: (num_envs, 3) - [vx, vy, omega_z]
            - vx, vy: Linear velocity to reach goal
            - omega_z: Always 0 (point directly at goal)
    
    Note: 
        - Goal position is stored in env.unwrapped.goal_positions if it exists
        - Otherwise, goal is set to current_pos + [goal_distance, 0, 0]
        - When robot reaches goal (distance < 0.5m), a new goal is set
    """
    # Get robot asset
    robot: Articulation = env.scene[asset_cfg.name]
    robot_pos = robot.data.root_pos_w[:, :2]  # (num_envs, 2) - [x, y]
    
    # Initialize goal positions if not exists
    if not hasattr(env.unwrapped, 'goal_positions'):
        env.unwrapped.goal_positions = robot_pos.clone()
        env.unwrapped.goal_positions[:, 0] += goal_distance  # X + goal_distance
        env.unwrapped.goal_distance = goal_distance
    
    goal_pos = env.unwrapped.goal_positions  # (num_envs, 2)
    
    # Calculate direction vector to goal
    direction = goal_pos - robot_pos  # (num_envs, 2)
    distance = torch.norm(direction, dim=1, keepdim=True)  # (num_envs, 1)
    
    # Check if any environment reached the goal
    goal_threshold = 0.5  # meters
    reached_goal = (distance.squeeze(1) < goal_threshold)
    
    # Reset goal for environments that reached it
    if reached_goal.any():
        reset_ids = torch.where(reached_goal)[0]
        new_pos = robot_pos[reset_ids]
        env.unwrapped.goal_positions[reset_ids, 0] = new_pos[:, 0] + goal_distance
        env.unwrapped.goal_positions[reset_ids, 1] = new_pos[:, 1]
        
        # Recalculate direction for reset environments
        direction = env.unwrapped.goal_positions - robot_pos
        distance = torch.norm(direction, dim=1, keepdim=True)
    
    # Normalize direction and scale to max_speed
    # Use distance-based speed scaling: slower when close to goal
    speed_scale = torch.clamp(distance / 2.0, 0.1, 1.0)  # Slow down within 2m, shape: (num_envs, 1)
    normalized_direction = direction / (distance + 1e-6)  # Avoid division by zero, shape: (num_envs, 2)
    
    # Calculate velocity commands in world frame
    vel_commands = torch.zeros(env.num_envs, 3, device=env.device)
    vel_commands[:, :2] = normalized_direction * max_speed * speed_scale  # Broadcasting: (num_envs, 2) * scalar * (num_envs, 1)
    vel_commands[:, 2] = 0.0  # No angular velocity (point directly at goal)
    
    return vel_commands


def teacher_semantic_cost_map(
    env: ManagerBasedRLEnv,
    lidar_cfg: SceneEntityCfg,
    height_scanner_cfg: SceneEntityCfg | None = None,  # Optional: LiDAR now provides height map
    command_name: str = "robot_velocity_command",
    asset_cfg: SceneEntityCfg | None = None,
) -> torch.Tensor:
    """Generate cost map from ground truth semantic labels (teacher mode).
    
    This observation function uses ground truth semantic labels from LiDAR
    instead of PVCNN inference. It's designed for training a teacher policy
    that has access to perfect semantic information.
    
    Args:
        env: The environment instance.
        lidar_cfg: Configuration for the semantic LiDAR sensor.
        height_scanner_cfg: Configuration for the height scanner sensor (optional, deprecated).
            LiDAR now provides height map directly via return_height_map=True.
        command_name: Name of the velocity command in env.command_manager.
        asset_cfg: Robot asset configuration (optional, for future use).
    
    Returns:
        Flattened cost map tensor: (num_envs, 32*32)
    """
    # Initialize cost map generator (singleton pattern)
    # Note: grid_size should match LiDAR's height_map_size/resolution
    # LiDAR config: height_map_size=(1.5, 1.5), resolution=0.1 -> 15x15 grid
    if not hasattr(teacher_semantic_cost_map, 'generator') or teacher_semantic_cost_map.generator is None:
        teacher_semantic_cost_map.generator = TeacherCostMapGenerator(
            device=env.device,
            grid_size=(15, 15),  # Match LiDAR height_map output: 1.5m / 0.1m = 15
            grid_resolution=0.1,
            x_range=(-0.75, 0.75),  # 1.5m / 2 = 0.75m
            y_range=(-0.75, 0.75),
        )
    
    # Get semantic LiDAR sensor
    from go2_pvcnn.sensor.lidar.semantic_lidar_sensor import SemanticLidarSensor
    lidar_sensor: SemanticLidarSensor = env.scene.sensors[lidar_cfg.name]
    
    # Get velocity command from command manager
    command_velocity = env.command_manager.get_command(command_name)  # (num_envs, 3)
    
    # Get LiDAR data
    # pointcloud: (num_envs, num_rays, 3) - [x, y, z] in robot frame (only yaw)
    # semantic_labels: (num_envs, num_rays) - semantic class IDs
    point_cloud = lidar_sensor.data.pointcloud  # (num_envs, num_rays, 3)
    semantic_labels = lidar_sensor.data.semantic_labels  # (num_envs, num_rays)
    
    # Ensure correct shapes
    assert point_cloud is not None, "LiDAR pointcloud is None. Set return_pointcloud=True in config."
    assert semantic_labels is not None, "Semantic labels is None. Set return_semantic_labels=True in config."
    
    # Handle semantic_labels format if needed
    if semantic_labels.dim() == 3:
        semantic_labels = semantic_labels.squeeze(-1)
    
    assert point_cloud.shape[:2] == semantic_labels.shape, \
        f"Shape mismatch: point_cloud {point_cloud.shape} vs semantic_labels {semantic_labels.shape}"
    
    # Get height map from LiDAR (not from separate height scanner)
    # LiDAR provides height_map via return_height_map=True
    if hasattr(lidar_sensor.data, 'height_map') and lidar_sensor.data.height_map is not None:
        height_map = lidar_sensor.data.height_map  # (num_envs, H, W)
        if not hasattr(teacher_semantic_cost_map, '_debug_printed'):
            print(f"[teacher_semantic_cost_map] height_map shape: {height_map.shape}")
            print(f"[teacher_semantic_cost_map] generator.grid_size: {teacher_semantic_cost_map.generator.grid_size}")
            teacher_semantic_cost_map._debug_printed = True
    else:
        # Fallback: use deprecated height_scanner if provided
        if height_scanner_cfg is not None:
            height_scanner: RayCaster = env.scene.sensors[height_scanner_cfg.name]
            height_data = height_scanner.data.ray_hits_w[..., -1]  # Extract Z coordinate
            # Use grid_size from generator
            h, w = teacher_semantic_cost_map.generator.grid_size
            height_map = height_data.reshape(-1, h, w)
        else:
            # No height map available, create zero height map
            num_envs = point_cloud.shape[0]
            h, w = teacher_semantic_cost_map.generator.grid_size
            height_map = torch.zeros(num_envs, h, w, 
                                    device=env.device, dtype=torch.float32)
    
    # Generate cost map using ground truth semantic labels
    cost_map = teacher_semantic_cost_map.generator.generate_cost_map(
        point_xyz=point_cloud,
        semantic_labels=semantic_labels,
        height_map=height_map,
        command_velocity=command_velocity,
    )
    
    # Debug: print shapes before combining
    if not hasattr(teacher_semantic_cost_map, '_shape_debug_printed'):
        print(f"[teacher_semantic_cost_map] Before unsqueeze:")
        print(f"  - cost_map shape: {cost_map.shape}")
        print(f"  - height_map shape: {height_map.shape}")
        teacher_semantic_cost_map._shape_debug_printed = True
    
    # Combine cost_map and height_map as 2-channel input for CNN
    # ActorCriticCNN expects [B, C, H, W] format
    # Channel 0: cost_map, Channel 1: height_map
    cost_map = cost_map.unsqueeze(1)  # (num_envs, 1, H, W)
    height_map = height_map.unsqueeze(1)  # (num_envs, 1, H, W)
    
    # Debug: verify shapes after unsqueeze
    if not hasattr(teacher_semantic_cost_map, '_concat_debug_printed'):
        print(f"[teacher_semantic_cost_map] After unsqueeze:")
        print(f"  - cost_map shape: {cost_map.shape}")
        print(f"  - height_map shape: {height_map.shape}")
        teacher_semantic_cost_map._concat_debug_printed = True
    
    dual_channel_input = torch.cat([cost_map, height_map], dim=1)  # (num_envs, 2, H, W)
    
    return dual_channel_input






