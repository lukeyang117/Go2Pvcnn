"""Observation functions for Go2 PVCNN locomotion."""

from __future__ import annotations

import torch
from typing import TYPE_CHECKING, Dict

from isaaclab.assets import Articulation
from isaaclab.managers import SceneEntityCfg
from isaaclab.sensors import RayCaster
from isaaclab.envs import mdp as isaac_mdp

if TYPE_CHECKING:
    from isaaclab.envs import ManagerBasedRLEnv

# Import cost map generator
from go2_pvcnn.mdp.cost_map import CostMapGenerator


def pvcnn_features_with_cost_map(
    env: ManagerBasedRLEnv,
    sensor_cfg: SceneEntityCfg = SceneEntityCfg("lidar_sensor"),
    height_scanner_cfg: SceneEntityCfg = SceneEntityCfg("height_scanner"),
) -> torch.Tensor:
    """Generate cost map from LiDAR semantic segmentation + height scanner.
    
    Data flow:
        LiDARç‚¹äº‘ â†’ PVCNNæ¨ç† â†’ è¯­ä¹‰åˆ†å‰²ç»“æœ + é«˜ç¨‹å›¾ â†’ ç”Ÿæˆä»£ä»·åœ°å›¾
    
    The cost map is generated by:
        1. PVCNN performs semantic segmentation on LiDAR point cloud
        2. Height scanner provides ground truth height map (16Ã—16 grid)
        3. CostMapGenerator combines semantic + height data:
            - Distance cost: proximity to obstacles (from semantic labels)
            - Gradient cost: terrain steepness (from height scanner)
            - Confidence cost: semantic uncertainty (from PVCNN confidence)
    
    Returns:
        cost_map: (batch, 256) - Flattened single-channel cost map (16Ã—16=256)
            - ç»¼åˆäº†éšœç¢ç‰©ã€è·ç¦»ã€æ¢¯åº¦ã€é«˜åº¦çš„åŠ æƒä»£ä»·
            - ç¬¦å·ç”±height_mapå†³å®šï¼šæ­£å€¼=éš¾èµ°ä¸Šå¡ï¼Œè´Ÿå€¼=éš¾èµ°ä¸‹å¡
    
    Note: This function stores point_cloud and semantic_labels in env for PPO training.
    """
    # Initialize debug counters and cost map generator
    if not hasattr(pvcnn_features_with_cost_map, '_debug_printed'):
        pvcnn_features_with_cost_map._debug_printed = False
        pvcnn_features_with_cost_map._call_count = 0
        
        # Initialize cost map generator (singleton)
        # Match height_scanner size: resolution=0.1, size=[1.5, 1.5] = 16Ã—16 grid (including boundaries)
        pvcnn_features_with_cost_map.cost_map_generator = CostMapGenerator(
            grid_size=(16, 16),  # åŒ¹é…height_scannerçš„ç½‘æ ¼å°ºå¯¸ (GridPatternCfgåŒ…å«è¾¹ç•Œç‚¹)
            grid_resolution=0.1,  # åŒ¹é…height_scannerçš„åˆ†è¾¨ç‡
            x_range=(-0.75, 0.75),  # size[0]/2 = 1.5/2 = 0.75
            y_range=(-0.75, 0.75),  # size[1]/2 = 1.5/2 = 0.75
            obstacle_class_ids=[1, 2, 3],  # Class 0=Terrain, 1=CrackerBox, 2=SugarBox, 3=TomatoSoupCan
            device=env.device,
            # ä»£ä»·æƒé‡å‚æ•°
            weight_obstacle=1.0,
            weight_distance=0.5,
            weight_gradient=1.0,
            weight_abs_height=0.3
        )
    
    # Increment call counter
    pvcnn_features_with_cost_map._call_count += 1
    
    # Get LiDAR sensor
    lidar_sensor: RayCaster = env.scene.sensors[sensor_cfg.name]
    
    # Debug: Print sensor type and configuration (only once)
    if not pvcnn_features_with_cost_map._debug_printed:
        print(f"\n{'='*80}")
        print(f"[PVCNN+CostMap DEBUG] LiDAR Sensor Configuration:")
        print(f"  - Sensor type: {type(lidar_sensor).__name__}")
        print(f"  - Number of rays: {lidar_sensor.num_rays}")
        print(f"  - Cost map grid: 16Ã—16 (matching height_scanner)")
        print(f"  - Cost map range: X=[-0.75, 0.75]m, Y=[-0.75, 0.75]m")
        print(f"  - Using Isaac Lab pointcloud API with yaw-alignment")
        
        # ========================================
        # ğŸ†• æ£€æŸ¥è¯­ä¹‰æ ‡ç­¾æ”¯æŒ
        # ========================================
        if hasattr(lidar_sensor.data, 'semantic_labels'):
            print(f"  - âœ… Semantic labels: SUPPORTED")
            if hasattr(lidar_sensor, 'semantic_class_names'):
                print(f"  - Semantic classes: {lidar_sensor.semantic_class_names}")
        
        print(f"{'='*80}\n")
        pvcnn_features_with_cost_map._debug_printed = True
    
    # Get point cloud data in yaw-aligned frame
    # Isaac Lab respects ray_alignment="yaw" and generates pointcloud accordingly
    point_cloud_raw = lidar_sensor.data.pointcloud  # (num_envs, num_rays, 3) - yaw-aligned point cloud
    
    # Debug: Check point cloud data and detect Inf values BEFORE any processing
    if pvcnn_features_with_cost_map._call_count % 100 == 1:
        print(f"\n[PVCNN DEBUG] Raw Point Cloud Data (BEFORE processing):")
        print(f"  - point_cloud_raw shape: {point_cloud_raw.shape}")
        print(f"  - Coordinate frame: yaw-aligned (from Isaac Lab pointcloud)")
        
        # Count Inf values in ORIGINAL data
        has_inf = torch.isinf(point_cloud_raw).any(dim=-1)  # (num_envs, num_rays)
        total_rays = has_inf.numel()
        inf_rays = has_inf.sum().item()
        print(f"  - Rays with Inf (ORIGINAL): {inf_rays}/{total_rays} ({100*inf_rays/total_rays:.2f}%)")
        
        # Check for valid hits
        valid_rays = total_rays - inf_rays
        if valid_rays > 0:
            # Get valid points for statistics
            valid_points = point_cloud_raw[~has_inf]
            print(f"  - Valid rays: {valid_rays} ({100*valid_rays/total_rays:.2f}%)")
            print(f"  - Valid points range: X[{valid_points[:,0].min():.2f}, {valid_points[:,0].max():.2f}], "
                  f"Y[{valid_points[:,1].min():.2f}, {valid_points[:,1].max():.2f}], "
                  f"Z[{valid_points[:,2].min():.2f}, {valid_points[:,2].max():.2f}]")
        
        # ========================================
        # ğŸ†• è¯­ä¹‰æ ‡ç­¾è°ƒè¯•ä¿¡æ¯
        # ========================================
        if hasattr(lidar_sensor.data, 'semantic_labels'):
            semantic_labels = lidar_sensor.data.semantic_labels #shape: (num_envs, num_rays)
            print(f"\n[SEMANTIC DEBUG] Semantic Labels Information:")
            print(f"  - semantic_labels shape: {semantic_labels.shape}")
            print(f"  - semantic_labels dtype: {semantic_labels.dtype}")
            print(f"  - Value range: [{semantic_labels.min().item()}, {semantic_labels.max().item()}]")
            
            # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„æ•°é‡
            unique_labels, counts = torch.unique(semantic_labels, return_counts=True)
            print(f"  - Unique labels found: {unique_labels.tolist()}")
            print(f"  - Label distribution:")
            for label, count in zip(unique_labels.tolist(), counts.tolist()):
                percentage = 100 * count / semantic_labels.numel()
                if label == -1:
                    label_name = "no_hit"
                elif label == 0:
                    label_name = "terrain"
                elif hasattr(lidar_sensor, 'semantic_class_names') and label < len(lidar_sensor.semantic_class_names):
                    label_name = lidar_sensor.semantic_class_names[label]
                else:
                    label_name = f"object_{label}"
                print(f"    - Label {label:2d} ({label_name:20s}): {count:6d} rays ({percentage:5.2f}%)")
            
            # æ£€æŸ¥ hit_mesh_source
            if hasattr(lidar_sensor.data, 'hit_mesh_source'):
                hit_mesh_source = lidar_sensor.data.hit_mesh_source
                print(f"\n  - hit_mesh_source shape: {hit_mesh_source.shape}")
                unique_sources, source_counts = torch.unique(hit_mesh_source, return_counts=True)
                print(f"  - Hit sources:")
                for source, count in zip(unique_sources.tolist(), source_counts.tolist()):
                    percentage = 100 * count / hit_mesh_source.numel()
                    source_name = {-1: "no_hit", 0: "static_mesh", 1: "dynamic_mesh"}.get(source, f"unknown_{source}")
                    print(f"    - Source {source:2d} ({source_name:15s}): {count:6d} rays ({percentage:5.2f}%)")
        else:
            print(f"\n[SEMANTIC DEBUG] âŒ semantic_labels not available in sensor data")
    
    # Isaac Lab's pointcloud respects ray_alignment="yaw" - already in yaw-aligned frame
    # Create comprehensive mask for valid points (not NaN, not Inf, not too close to origin)
    valid_mask = (~torch.isnan(point_cloud_raw).any(dim=-1) & 
                  ~torch.isinf(point_cloud_raw).any(dim=-1))  # (num_envs, num_rays)
    
    # Filter out points too close to origin (likely invalid, at least 10cm)
    distance_from_origin = torch.norm(point_cloud_raw, dim=2)
    valid_mask = valid_mask & (distance_from_origin > 0.1)
    
    method_used = "Isaac Lab pointcloud (yaw-aligned)"
    
    # Debug statistics (print every 100 calls)
    if pvcnn_features_with_cost_map._call_count % 100 == 1:
        valid_count = valid_mask.sum(dim=1)
        total_rays = point_cloud_raw.shape[1]
        
        print(f"\n[PVCNN DEBUG - Call {pvcnn_features_with_cost_map._call_count}] Point Cloud Filtering:")
        print(f"  - Method: {method_used}")
        print(f"  - Total rays: {total_rays}")
        print(f"  - Valid points per env: min={valid_count.min()}, max={valid_count.max()}, mean={valid_count.float().mean():.1f}")
        
        # Statistics on valid points only
        all_valid_points = point_cloud_raw[valid_mask]
        if all_valid_points.shape[0] > 0:
            print(f"  - Valid points range: X[{all_valid_points[:,0].min():.3f}, {all_valid_points[:,0].max():.3f}], "
                  f"Y[{all_valid_points[:,1].min():.3f}, {all_valid_points[:,1].max():.3f}], "
                  f"Z[{all_valid_points[:,2].min():.3f}, {all_valid_points[:,2].max():.3f}]")
    
    # ========================================
    # Sample valid points to target size (2046 points)
    # ========================================
    target_num_points = 2046
    batch_size = point_cloud_raw.shape[0]
    
    # åŒæ—¶å¤„ç†ç‚¹äº‘å’Œè¯­ä¹‰æ ‡ç­¾ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
    raw_semantic_labels = None
    if hasattr(lidar_sensor.data, 'semantic_labels'):
        raw_semantic_labels = lidar_sensor.data.semantic_labels  # (num_envs, num_rays)
    
    # Process each environment separately to handle different valid point counts
    processed_point_clouds = []
    processed_labels = []
    
    for b in range(batch_size):
        valid_points = point_cloud_raw[b][valid_mask[b]]  # ç›´æ¥ä»rawæå–validç‚¹ (num_valid, 3)
        num_valid = valid_points.shape[0]
        
        # åŒæ—¶æå–valid semantic labels
        if raw_semantic_labels is not None:
            valid_labels = raw_semantic_labels[b][valid_mask[b]]  # (num_valid,)
        
        if num_valid == 0:
            # No valid points - create dummy points in a small sphere
            dummy_points = torch.randn(target_num_points, 3, device=point_cloud_raw.device) * 0.5
            processed_point_clouds.append(dummy_points)
            
            if raw_semantic_labels is not None:
                dummy_labels = torch.zeros(target_num_points, dtype=torch.long, device=point_cloud_raw.device)
                processed_labels.append(dummy_labels)
                
        elif num_valid < target_num_points:
            # Not enough points: duplicate existing points
            num_repeats = (target_num_points + num_valid - 1) // num_valid
            repeated_points = valid_points.repeat(num_repeats, 1)[:target_num_points]
            processed_point_clouds.append(repeated_points)
            
            if raw_semantic_labels is not None:
                repeated_labels = valid_labels.repeat(num_repeats)[:target_num_points]
                processed_labels.append(repeated_labels)
            
        
        elif num_valid > target_num_points:
            # Too many points: use farthest point sampling
            distances = torch.norm(valid_points, dim=1)
            _, furthest_indices = torch.topk(distances, k=target_num_points, largest=True)
            sampled_points = valid_points[furthest_indices]
            processed_point_clouds.append(sampled_points)
            
            if raw_semantic_labels is not None:
                sampled_labels = valid_labels[furthest_indices]
                processed_labels.append(sampled_labels)
            
        
        else:
            # Exactly right number
            processed_point_clouds.append(valid_points)
            if raw_semantic_labels is not None:
                processed_labels.append(valid_labels)
    
    # Stack all environments
    point_cloud = torch.stack(processed_point_clouds, dim=0)  # (batch, target_num_points, 3)
    
    # Stack semantic labels if available
    semantic_labels = None
    if raw_semantic_labels is not None:
        semantic_labels = torch.stack(processed_labels, dim=0)  # (batch, target_num_points)
    
    if pvcnn_features_with_cost_map._call_count % 100 == 1:
        print(f"  - Final point cloud shape: {point_cloud.shape}")
        print(f"  - Final stats: min={point_cloud.min():.3f}, max={point_cloud.max():.3f}, mean={point_cloud.mean():.3f}")
        # Check for remaining invalid values
        if torch.isnan(point_cloud).any() or torch.isinf(point_cloud).any():
            print(f"  [ERROR] Still have NaN/Inf after processing!")
        
        if semantic_labels is not None:
            print(f"\n[SEMANTIC DEBUG] Processed semantic labels:")
            print(f"  - Shape: {semantic_labels.shape}")
            print(f"  - Unique labels: {torch.unique(semantic_labels).tolist()}")
            for label_id in torch.unique(semantic_labels):
                count = (semantic_labels == label_id).sum().item()
                percentage = 100 * count / semantic_labels.numel()
                print(f"    - Label {label_id}: {count} points ({percentage:.2f}%)")
    
    # Point cloud ready for PVCNN (XYZ only, 3 channels)
    # Model adapted to accept 3-channel input
    
    # Get PVCNN wrapper from unwrapped environment (injected by RslRlPvcnnEnvWrapper)
    # NOTE: During gym.make(), the environment may call reset() before wrapper is injected
    # So we need to handle the case where wrapper doesn't exist yet
    pvcnn_wrapper = getattr(env.unwrapped, 'pvcnn_wrapper', None)
    
    if pvcnn_wrapper is not None:
        if pvcnn_features_with_cost_map._call_count % 100 == 1:
            print(f"[PVCNN DEBUG] Using PVCNN wrapper from env.unwrapped.pvcnn_wrapper")
            print(f"[PVCNN DEBUG] Wrapper device: {pvcnn_wrapper.device}")
            print(f"[PVCNN DEBUG] Point cloud device: {point_cloud.device}")
        
        # CRITICAL: Ensure point cloud is contiguous in memory to prevent CUDA errors
        if not point_cloud.is_contiguous():
            point_cloud = point_cloud.contiguous()
        
        # Extract features using PVCNN (this gives us per-point semantic segmentation)
        with torch.no_grad():
            semantic_features = pvcnn_wrapper.extract_features(point_cloud)  # (batch, num_points, num_classes)
        
        # Debug: Print PVCNN output statistics
        if pvcnn_features_with_cost_map._call_count % 100 == 1:
            print(f"\n[PVCNN DEBUG] PVCNN Semantic Segmentation:")
            print(f"  - Shape: {semantic_features.shape}")
            print(f"  - Range: [{semantic_features.min():.6f}, {semantic_features.max():.6f}]")
            print(f"  - Mean: {semantic_features.mean():.6f}, Std: {semantic_features.std():.6f}")
        
        # ========================================
        # Get Height Map from Height Scanner
        # ========================================
        
        # Get height scan from height scanner sensor (16Ã—16 grid)
        height_scan = isaac_mdp.height_scan(env, sensor_cfg=height_scanner_cfg)  # (batch, 256)
        height_map_2d = height_scan.reshape(env.num_envs, 16, 16)  # (batch, H, W)
        
        if pvcnn_features_with_cost_map._call_count % 100 == 1:
            print(f"\n[HEIGHT SCANNER DEBUG] Height Map:")
            print(f"  - Shape: {height_map_2d.shape}")
            print(f"  - Range: [{height_map_2d.min():.3f}, {height_map_2d.max():.3f}]")
            print(f"  - Mean: {height_map_2d.mean():.3f}, Std: {height_map_2d.std():.3f}")
        
        # ========================================
        # Generate Cost Map from PVCNN output + Height Map
        # ========================================
        
        # Transpose semantic features to (batch, num_classes, num_points)
        semantic_logits = semantic_features.transpose(1, 2)
        
        # Compute semantic confidence and predicted classes
        semantic_probs = torch.softmax(semantic_logits, dim=1)
        semantic_confidence, pred_classes = semantic_probs.max(dim=1)  # (batch, num_points), (batch, num_points)
        
        # Generate cost map using the CostMapGenerator
        # CostMapGenerator will:
        # 1. Project 3D points to 2D grid using point_xyz (x, y) matching height_scanner range
        # 2. Use height_map from height scanner to compute gradient cost
        # 3. Use pred_classes to identify obstacles and compute distance cost
        # 4. Use semantic_confidence to compute uncertainty cost
        cost_map_2d = pvcnn_features_with_cost_map.cost_map_generator.generate_cost_map(
            point_xyz=point_cloud,  # (batch, num_points, 3)
            pred_classes=pred_classes,  # (batch, num_points) - predicted class indices
            semantic_confidence=semantic_confidence,  # (batch, num_points)
            height_map=height_map_2d,  # (batch, H, W) - from height scanner
        )
        
        # Flatten cost_map (batch, H, W) -> (batch, H*W)
        cost_map_flat = cost_map_2d.reshape(cost_map_2d.shape[0], -1)
        
        if pvcnn_features_with_cost_map._call_count % 100 == 1:
            print(f"\n[CostMap DEBUG] Generated Cost Map:")
            print(f"  - 2D shape: {cost_map_2d.shape}")
            print(f"  - Flattened shape: {cost_map_flat.shape}")
            print(f"  - Range: [{cost_map_flat.min():.6f}, {cost_map_flat.max():.6f}]")
            print(f"  - Mean: {cost_map_flat.mean():.6f}, Std: {cost_map_flat.std():.6f}")
        
        # Store point cloud and semantic labels for PPO training
        env.unwrapped.last_point_cloud = point_cloud.detach()
        env.unwrapped.last_semantic_labels = semantic_labels.detach() if semantic_labels is not None else None
        
        # Return only the cost map (not PVCNN features)
        return cost_map_flat
    else:
        # PVCNN wrapper not found - this should not happen if wrapper is properly injected
        print(f"[ERROR] PVCNN wrapper not found in env.unwrapped.pvcnn_wrapper!")
        
        # Fallback: return zeros for cost_map only
        cost_map_dim = 16 * 16  # Single channel * 16 * 16 grid = 256
        
        env.unwrapped.last_point_cloud = None
        env.unwrapped.last_semantic_labels = None
        
        return torch.zeros(env.num_envs, cost_map_dim, device=env.device)






